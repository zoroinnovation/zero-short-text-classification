{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46beda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets evaluate accelerate -q   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e65d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn \n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f64914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import TrainingArguments\n",
    "help(TrainingArguments) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, logging, numpy as np, pandas as pd, matplotlib.pyplot as plt, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import ( \n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility & Paths\n",
    "\n",
    "set_seed(42)\n",
    "OUTPUT_DIR = \"outputs/sst2\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(OUTPUT_DIR, \"training.log\"),\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logging.info(\"Starting run\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba78fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLUE/SST-2 dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize_fn, batched=True)\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds   = dataset[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ca044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435db29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    labels = eval_pred.label_ids\n",
    "    preds  = np.argmax(eval_pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Args\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUTPUT_DIR, \"results\"),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a072de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52105a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "train_out = trainer.train()\n",
    "logging.info(\"Training finished\")\n",
    "\n",
    "# Save log history\n",
    "log_history = trainer.state.log_history\n",
    "pd.DataFrame(log_history).to_csv(os.path.join(OUTPUT_DIR, \"training_log_history.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8549e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    " \n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation:\", eval_results)\n",
    "with open(os.path.join(OUTPUT_DIR, \"eval_results.json\"), \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions & Confusion Matrix\n",
    "\n",
    "preds_output = trainer.predict(val_ds) \n",
    "y_true = preds_output.label_ids\n",
    "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix - SST-2 (BERT)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "classes = [\"Negative\",\"Positive\"]\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], \"d\"),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=200)\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d90d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Convert validation split to pandas DataFrame\n",
    "val_df = dataset[\"validation\"].to_pandas()\n",
    "\n",
    "# Save sample predictions (first 50 validation rows)\n",
    "sample_df = pd.DataFrame({\n",
    "    \"sentence\": val_df[\"sentence\"].iloc[:50],\n",
    "    \"true_label\": val_df[\"labels\"].iloc[:50],\n",
    "    \"pred_label\": y_pred[:50]\n",
    "})\n",
    "\n",
    "# Save as CSV\n",
    "sample_df.to_csv(r\"D:\\Zoro_project2\\zero-short-text-classification\\outputs\\sst2\\sample_predictions.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loss Plot\n",
    "\n",
    "hist_df = pd.DataFrame(log_history)\n",
    "train_loss_df = hist_df[hist_df[\"loss\"].notna()][[\"step\", \"loss\"]].drop_duplicates(subset=[\"step\"])\n",
    "fig = plt.figure(figsize=(7,4))\n",
    "plt.plot(train_loss_df[\"step\"], train_loss_df[\"loss\"])\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss vs Step\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"training_loss_curve.png\"), dpi=200)\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472de908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Metrics over Epochs\n",
    "\n",
    "metrics_cols = [\"eval_accuracy\", \"eval_precision\", \"eval_recall\", \"eval_f1\", \"epoch\"]\n",
    "eval_hist = hist_df.dropna(subset=[\"epoch\"])\n",
    "eval_hist = eval_hist[[c for c in metrics_cols if c in eval_hist.columns]].drop_duplicates(subset=[\"epoch\"])\n",
    "\n",
    "for metric in [\"eval_accuracy\", \"eval_precision\", \"eval_recall\", \"eval_f1\"]:\n",
    "    if metric in eval_hist.columns:\n",
    "        fig = plt.figure(figsize=(6,4))\n",
    "        plt.plot(eval_hist[\"epoch\"], eval_hist[metric], marker=\"o\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(metric.replace(\"eval_\", \"\").title())\n",
    "        plt.title(f\"{metric.replace('eval_', '').upper()} vs Epoch\")\n",
    "        plt.xticks(eval_hist[\"epoch\"])\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{metric}_vs_epoch.png\"), dpi=200)\n",
    "        plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of final eval metrics\n",
    "\n",
    "final_metrics = {  \n",
    "    \"Accuracy\":   float(eval_results.get(\"eval_accuracy\", 0.0)),\n",
    "    \"Precision\":  float(eval_results.get(\"eval_precision\", 0.0)),  \n",
    "    \"Recall\":     float(eval_results.get(\"eval_recall\", 0.0)),\n",
    "    \"F1\":         float(eval_results.get(\"eval_f1\", 0.0)),\n",
    "}\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.bar(list(final_metrics.keys()), list(final_metrics.values()))\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Final Evaluation Metrics (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"final_metrics_bar.png\"), dpi=200)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\n",
    "    f\" Done! All outputs saved in {OUTPUT_DIR}/\\n\"\n",
    "    \"- training.log\\n\"\n",
    "    \"- training_log_history.csv\\n\"\n",
    "    \"- eval_results.json\\n\"\n",
    "    \"- confusion_matrix.png\\n\"\n",
    "    \"- sample_predictions.csv\\n\"\n",
    "    \"- training_loss_curve.png\\n\"\n",
    "    \"- eval_accuracy_vs_epoch.png (and precision/recall/f1 variants)\\n\"\n",
    "    \"- final_metrics_bar.png\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py_3)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
